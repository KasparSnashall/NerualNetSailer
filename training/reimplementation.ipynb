{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reimplementing basics of tensorflow\n",
    "Exersise to understand how tensorflow is built, again based on the book by Chollet. In the 2021 book version there are some coding issues on page 63. Corrected in this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveDense:\n",
    "    \"\"\"\n",
    "    A basic dense layer, requires an activation function\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.activation = activation\n",
    "        w_shape = (input_size, output_size) # weights tuple shape\n",
    "        w_initial_vals = tf.random.uniform(w_shape, minval=0, maxval=0.1)\n",
    "        self.W = tf.Variable(w_initial_vals)\n",
    "        \n",
    "        b_shape = (output_size,) # obviously a vector not a matrix output\n",
    "        b_intial_vals = tf.zeros(b_shape)\n",
    "        self.b = tf.Variable(b_intial_vals)\n",
    "    \n",
    "    def  __call__(self,inputs):\n",
    "        return self.activation(tf.matmul(inputs, self.W)+ self.b)\n",
    "    \n",
    "    @property # not used to this decorator\n",
    "    def weights(self):\n",
    "        return [self.W, self.b]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveSequential:\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            weights += layer.weights\n",
    "        return weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequential just applys each layer in sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveSequential([NaiveDense(28*28, 512, tf.nn.relu),NaiveDense(512, 10, tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model.weights) == 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty simple to create custom neural nets obivously the speed disadvantage ultimately makes this solution not worthwhile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BatchGenerator:\n",
    "    def __init__(self, images, labels, batch_size = 128):\n",
    "        assert len(images) == len(labels) # should probably be a raise exception\n",
    "        self.index = 0\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = math.ceil(len(images)/batch_size)\n",
    "        \n",
    "    def next(self):\n",
    "        images = self.images[self.index : self.index + self.batch_size]\n",
    "        labels = self.labels[self.index : self.index + self.batch_size]\n",
    "        self.index += self.batch_size\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_training_step(model, images_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images_batch)\n",
    "        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(labels_batch, predictions)\n",
    "        aver_loss = tf.reduce_mean(per_sample_losses)\n",
    "    gradients = tape.gradient(aver_loss, model.weights)\n",
    "    update_weights(gradients, model.weights)\n",
    "    return aver_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "def update_weights(gradients, weights):\n",
    "    for grad, weight in zip(gradients, weights):\n",
    "        weight.assign_sub(grad*learning_rate)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, images, labels, epochs, batch_size=128):\n",
    "    for epoch_counter in range(epochs):\n",
    "        print(f\"Epoch # {epoch_counter}\")\n",
    "        batch_gen = BatchGenerator(images, labels)\n",
    "        for batch_counter in range(batch_gen.num_batches):\n",
    "            images_batch, labels_batch = batch_gen.next()\n",
    "            loss = one_training_step(model, images_batch, labels_batch)\n",
    "            if batch_counter % 100 == 0:\n",
    "                print(f\"loss at batch #{batch_counter}:{loss:.2f}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0\n",
      "loss at batch #0:4.45\n",
      "loss at batch #100:2.28\n",
      "loss at batch #200:2.25\n",
      "loss at batch #300:2.11\n",
      "loss at batch #400:2.26\n",
      "Epoch # 1\n",
      "loss at batch #0:1.94\n",
      "loss at batch #100:1.90\n",
      "loss at batch #200:1.86\n",
      "loss at batch #300:1.73\n",
      "loss at batch #400:1.85\n",
      "Epoch # 2\n",
      "loss at batch #0:1.61\n",
      "loss at batch #100:1.60\n",
      "loss at batch #200:1.53\n",
      "loss at batch #300:1.44\n",
      "loss at batch #400:1.52\n",
      "Epoch # 3\n",
      "loss at batch #0:1.35\n",
      "loss at batch #100:1.35\n",
      "loss at batch #200:1.25\n",
      "loss at batch #300:1.22\n",
      "loss at batch #400:1.28\n",
      "Epoch # 4\n",
      "loss at batch #0:1.15\n",
      "loss at batch #100:1.17\n",
      "loss at batch #200:1.05\n",
      "loss at batch #300:1.05\n",
      "loss at batch #400:1.11\n",
      "Epoch # 5\n",
      "loss at batch #0:1.00\n",
      "loss at batch #100:1.03\n",
      "loss at batch #200:0.91\n",
      "loss at batch #300:0.93\n",
      "loss at batch #400:0.99\n",
      "Epoch # 6\n",
      "loss at batch #0:0.88\n",
      "loss at batch #100:0.92\n",
      "loss at batch #200:0.81\n",
      "loss at batch #300:0.84\n",
      "loss at batch #400:0.90\n",
      "Epoch # 7\n",
      "loss at batch #0:0.80\n",
      "loss at batch #100:0.83\n",
      "loss at batch #200:0.73\n",
      "loss at batch #300:0.77\n",
      "loss at batch #400:0.83\n",
      "Epoch # 8\n",
      "loss at batch #0:0.73\n",
      "loss at batch #100:0.77\n",
      "loss at batch #200:0.66\n",
      "loss at batch #300:0.71\n",
      "loss at batch #400:0.78\n",
      "Epoch # 9\n",
      "loss at batch #0:0.68\n",
      "loss at batch #100:0.71\n",
      "loss at batch #200:0.61\n",
      "loss at batch #300:0.67\n",
      "loss at batch #400:0.74\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images,test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000,28*28)) \n",
    "train_images = train_images.astype(\"float32\")/255  \n",
    "test_images = test_images.reshape((10000,28*28)) \n",
    "test_images = test_images.astype(\"float32\")/255 \n",
    "fit(model, train_images, train_labels, 10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow so slow!!!! Glad keras is optimised."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
